{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "native-uncle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manually leveling\n"
     ]
    }
   ],
   "source": [
    "print('manually leveling')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77abad",
   "metadata": {},
   "source": [
    "## Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b601e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib.pyplot import imsave, imread\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ad1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from floodfill import *\n",
    "from dataloader import *\n",
    "from model import *\n",
    "from oracle import *\n",
    "from unet import *\n",
    "import ternausnet.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf5c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vivek.model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81907ee",
   "metadata": {},
   "source": [
    "## Testing Jupyter Notebook Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc3727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your name: alina\n",
      "Your name is: alina.\n"
     ]
    }
   ],
   "source": [
    "#Testing live input from user\n",
    "users_name = input(\"what is your name: \")\n",
    "print(f\"Your name is: {users_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "acfdec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 256, 256)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tests loading an image from a directory\n",
    "im_dir = \"/usr/xtmp/mammo/image_datasets/data_split_july2021/square_ROI_by_shape_segmentations_unbin/train/\"\n",
    "im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_03_10_a/Iter1/UNetSegmentations_ff/\"\n",
    "#im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/UNetSegmentations/Iter0_ff/Oval/\"\n",
    "#im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_11_29_a/Iter2/UNetSegmentations_C/Oval/\"\n",
    "#im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_11_29_a/Iter2/UNetSegmentations_C/Round/\"\n",
    "# im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_11_24_b/Iter1/UNetSegmentations_C/Oval/\"\n",
    "#im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/UNetSegmentations/Iter0_ff/Round/\"\n",
    "#ims = [\"DP_ACRF_18645_1.npy\"]\n",
    "#im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_02_10_a/Iter1/OracleThresholdedImages_ff/Irregular/\"\n",
    "ims = [\"Oval/DP_AGYG_113509_1.npy\"]\n",
    "\n",
    "for im in ims:\n",
    "    path = im_dir + im;\n",
    "    arr = np.load(path)\n",
    "    #arr = np.where(arr > 0.2,1,0)\n",
    "    print(arr.shape)\n",
    "    \n",
    "plt.imshow(arr[1],cmap='jet')\n",
    "#plt.show()\n",
    "im_dir = save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894ee31",
   "metadata": {},
   "source": [
    "## Active Learning: Setup - Run once per run id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb5d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"01_01_b\" #Format: Month_Day_(Run # that day by letter)\n",
    "#run_id = \"02_10_a\"\n",
    "#run_id = \"02_28_a\"\n",
    "#original \n",
    "#01_01_b is where we load from alina's oracle results\n",
    "#02_28_a is where we load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ab0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f60f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dir = \"/usr/xtmp/mammo/image_datasets/data_split_july2021/square_ROI_by_shape_segmentations_unbin/train/\"\n",
    "#im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_02_28_a/Iter1/OracleThresholdedImages_ff/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487ea26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Gets the dataloader. Passes in directory of ALL images\n",
    "#im_dir = \"/usr/xtmp/mammo/image_datasets/data_split_july2021/square_ROI_by_shape_segmentations/train/\"\n",
    "#GET THE NEW IM_DIR FOR SUBSEQUENT ITERATIONS\n",
    "im_dir = \"/usr/xtmp/mammo/image_datasets/data_split_july2021/square_ROI_by_shape_segmentations_unbin/train/\"\n",
    "dataloader = get_DataLoader(im_dir,32,2)\n",
    "\n",
    "#Tests the dataloader\n",
    "for i in tqdm(dataloader):\n",
    "#     print(max(i[1]))\n",
    "    print(i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "765929c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing info arrays for active learning. oracle_results maps patientIDs to 0,1 labels\n",
    "oracle_results = {}\n",
    "oracle_results_thresholds = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e48a9b",
   "metadata": {},
   "source": [
    "## Active Learning: Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9568e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initializes and trains the model. Plots a loss function of the initial training\n",
    "model,loss_tracker,criterion,optimizer = initialize_and_train_model(dataloader, epochs=5) #default batch_size and epochs\n",
    "plt.plot(loss_tracker) #plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b7b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patient_scores = []\n",
    "\n",
    "#right now patient_scores is initial scores for each patient (without active learning training)\n",
    "patient_scores = get_patient_scores(model,dataloader) \n",
    "all_patient_scores.append(patient_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006f471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_oracle_results, new_oracle_results_thresholds = query_oracle(oracle_results,oracle_results_thresholds,\n",
    "                                                                 patient_scores,im_dir,query_method=\"percentile=0.8\",\n",
    "                                                                 query_number=20)\n",
    "oracle_results, oracle_results_thresholds = new_oracle_results, new_oracle_results_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_num+=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_scores = get_patient_scores(model,dataloader)\n",
    "all_patient_scores.append(patient_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff0ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: track model loss somehow along with patient_scores\n",
    "for i in range(1):\n",
    "    model = model_update(model,dataloader,oracle_results,criterion,optimizer,num_epochs=5)\n",
    "\n",
    "    patient_scores = get_patient_scores(model,dataloader)\n",
    "    all_patient_scores.append(patient_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bded4f",
   "metadata": {},
   "source": [
    "## Metrics and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f48fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints out all patient scores.\n",
    "for i in all_patient_scores:\n",
    "    print(calculate_dispersion_metric(i,oracle_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the disperson metric\n",
    "j = []\n",
    "for i in all_patient_scores:\n",
    "    j.append(calculate_dispersion_metric(i,oracle_results))\n",
    "    \n",
    "plt.plot(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52692eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of patient scores: \" + str(len(patient_scores)))\n",
    "print(\"Length of oracle results: \" + str(len(oracle_results)))\n",
    "\n",
    "scores = []\n",
    "for key in patient_scores.keys():\n",
    "    scores.append(patient_scores[key])\n",
    "plt.plot(scores)\n",
    "\n",
    "ones = 0\n",
    "for i in oracle_results.keys():\n",
    "    if oracle_results[i]==1:\n",
    "        ones+=1\n",
    "print(\"Number of ones in oracle results: \", ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba948cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e8917f",
   "metadata": {},
   "source": [
    "Jump back to oracle query if you want to query more. Move on to retrain segmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cfb1f5",
   "metadata": {},
   "source": [
    "## Saving or Loading Oracle Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    #SAVE ORACLE\n",
    "\n",
    "    #make filepath\n",
    "    if users_name == 'alina':\n",
    "        save_dir = \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/CorrectSegmentations/\"\n",
    "    elif users_name == 'vaibhav':\n",
    "        save_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/CorrectSegmentations/\"\n",
    "\n",
    "    saved_oracle_filepaths = save_oracle_results(oracle_results,oracle_results_thresholds,im_dir,save_dir)\n",
    "    fpath = save_dir + \"saved_data_struct/\"\n",
    "    if not os.path.exists(fpath):\n",
    "        os.makedirs(fpath)\n",
    "    saved_oracle_filepaths_filepath = save_dir + \"saved_data_struct/Oracle_Filepaths.pickle\"\n",
    "    pickle.dump(saved_oracle_filepaths,open(saved_oracle_filepaths_filepath,\"wb\"))\n",
    "    pickle.dump(oracle_results,open(save_dir + \"saved_data_struct/Oracle_Results.pickle\",\"wb\"))\n",
    "    pickle.dump(oracle_results_thresholds,open(save_dir + \"saved_data_struct/Oracle_Results_Thresholds.pickle\",\"wb\"))\n",
    "    #TODO: Move border/weight creation to this method and out of convert_directory_to_floodfill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34c9d8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saved_oracle_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce7e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: #done running if False\n",
    "    #LOAD oracle_results and save_oracle_filepaths\n",
    "    if users_name == 'alina':\n",
    "        save_dir = \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/CorrectSegmentations/\"\n",
    "    elif users_name == 'vaibhav':\n",
    "        save_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/CorrectSegmentations/\"\n",
    "    saved_oracle_filepaths_filepath = save_dir + \"saved_data_struct/Oracle_Filepaths.pickle\"\n",
    "    #Reload from previous iteration\n",
    "    oracle_results = pickle.load(open(save_dir + \"saved_data_struct/Oracle_Results.pickle\",\"rb\"))\n",
    "    oracle_results_thresholds = pickle.load(open(save_dir + \"saved_data_struct/Oracle_Results_Thresholds.pickle\",\"rb\"))\n",
    "    saved_oracle_filepaths = pickle.load(open(saved_oracle_filepaths_filepath,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2eb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # RECOVER an accidentally delted oracle (recovers correct segmentations only)\n",
    "    oracle_results = {}\n",
    "    if users_name == 'alina':\n",
    "        recover_from_dir = \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/CorrectSegmentations/\"\n",
    "    for root, dirs, files in os.walk(recover_from_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".npy\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                print(filepath.split('/')[])\n",
    "                break\n",
    "                oracle_results[file[:-4]] = 1\n",
    "\n",
    "    # pickle.dump(oracle_results,open(recover_from_dir + \"saved_data_struct/Oracle_Results.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2aa7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all 0's from oracle_results.\n",
    "oracle_results = remove_bad_oracle_results(oracle_results)\n",
    "#NOTE: Don't FULLY reset oracle_results EVER during a run, only do above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8905a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "print(len(oracle_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee83d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_01_01_b/Iter1/CorrectSegmentations/\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "# #Convert oracle_results to oracle filepaths\n",
    "# all_oracle_filepaths = []\n",
    "    \n",
    "# filestem = '/'.join(saved_oracle_filepaths[0].split(\"/\")[:-2]) +\"/\"\n",
    "# print(filestem)\n",
    "\n",
    "# for key in oracle_results.keys():\n",
    "#     all_oracle_filepaths.append(filestem + key + \".npy\")\n",
    "\n",
    "# print(len(oracle_all_filepaths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d52ed2",
   "metadata": {},
   "source": [
    "## Retrain UNet and Save New Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bf75808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retoggle run id if we want to save today's set instead of loading in an old set.\n",
    "if True:\n",
    "    run_id = \"03_17_a\" \n",
    "    \n",
    "#Retoggle user to Vaibhav if I want to save \n",
    "if True:\n",
    "    users_name = \"vaibhav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3807b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to overwrite this directory? Type y or yes to continuey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [00:08<00:00, 102.70it/s]\n",
      "100%|██████████| 824/824 [05:18<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in /usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_03_17_a/Iter1/OracleThresholdedImages_ff/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Threshold masks based on oracle and save if needed.\n",
    "#If already done, then skip\n",
    "threshold_and_save_flag = True\n",
    "if threshold_and_save_flag:\n",
    "    #Save thresholded\n",
    "    if users_name == 'alina':\n",
    "        save_dir = \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/OracleThresholdedImages/\"\n",
    "    elif users_name == 'vaibhav':\n",
    "        save_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/OracleThresholdedImages/\"\n",
    "    else:\n",
    "        print(\"wrong username\")\n",
    "        \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    else:\n",
    "        user_input = input(\"Do you want to overwrite this directory? Type y or yes to continue\")\n",
    "        if not (user_input==\"y\" or user_input==\"yes\"):\n",
    "            assert(False)\n",
    "    #find all filepaths in im_dir\n",
    "    all_filepaths = []\n",
    "    for root, dirs, files in os.walk(im_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".npy\"):\n",
    "                all_filepaths.append(os.path.join(root, file))\n",
    "    threshold_and_save_images(all_filepaths, oracle_results_thresholds, save_dir)\n",
    "    save_dir = convert_directory_to_floodfill(save_dir,iter0=False)\n",
    "    print(f\"Saved in {save_dir}\")\n",
    "    unbin_im_dir = im_dir\n",
    "    im_dir = save_dir #im_dir is now replaced with the new directory\n",
    "    #save_dir is defunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82bb3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_dataloader(train_images_filepaths,batch_size,num_workers):\n",
    "    transforms_arr = [transforms.ToTensor(),transforms.Resize((256,256))]\n",
    "    transform = transforms.Compose(transforms_arr)\n",
    "    trainset = InhouseGetData(train_images_filepaths,transform,data_aug=True,has_weights=True)\n",
    "    trainloader = DataLoader(trainset,batch_size=batch_size,num_workers=num_workers)\n",
    "    return trainloader\n",
    "\n",
    "class InhouseGetData(Dataset):\n",
    "    def __init__(self,image_filepaths,image_transform,data_aug=True,has_weights=True):\n",
    "        super().__init__()\n",
    "        self.image_filepaths = image_filepaths\n",
    "        self.image_transform = image_transform\n",
    "        self.data_aug = data_aug\n",
    "        self.has_weights = has_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filepaths)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        filepath = self.image_filepaths[idx]\n",
    "        #print(\"Filepath: \" + filepath)\n",
    "        arr_and_mask = np.load(filepath)\n",
    "        copy_arr_mask = arr_and_mask.copy()\n",
    "        if self.data_aug:\n",
    "            copy_arr_mask = random_flip(copy_arr_mask, 0, True)\n",
    "            copy_arr_mask = random_flip(copy_arr_mask, 1, True)\n",
    "            copy_arr_mask = random_rotate_90(copy_arr_mask, True)\n",
    "            copy_arr_mask = random_rotate_90(copy_arr_mask, True)\n",
    "            copy_arr_mask = random_rotate_90(copy_arr_mask, True)\n",
    "        arr = copy_arr_mask[0,:,:].copy()\n",
    "        mask = copy_arr_mask[1,:,:].copy()\n",
    "        if self.has_weights:\n",
    "            weights = copy_arr_mask[2,:,:].copy()\n",
    "        arr = exposure.equalize_hist(arr) #histogram equalization, remove if u want\n",
    "        #arr = np.stack([arr,arr,arr])\n",
    "        #mask = np.stack([mask,mask,mask])\n",
    "        #no need to preprocess\n",
    "        #print(\"INSIDE\")\n",
    "        #print(arr.shape)\n",
    "        \n",
    "        image = self.image_transform(arr)\n",
    "        #image = our_transform(image)\n",
    "        #print(image.shape)\n",
    "        #print(\"OUTSIDE\")\n",
    "        mask_label = self.image_transform(mask)\n",
    "        #mask_label = our_transform(mask_label)\n",
    "        if self.has_weights:\n",
    "            weights_label = self.image_transform(weights)\n",
    "            #weights_label = our_transform(weights_label)\n",
    "        #a transform\n",
    "        # print(arr.shape)\n",
    "        # print(mask.shape)\n",
    "        \n",
    "        # transformed = self.a_transform(image=arr, mask=mask)\n",
    "        # a_image = transformed[\"image\"]\n",
    "        # a_mask = transformed[\"mask\"]\n",
    "        if self.has_weights:\n",
    "            return image,mask_label,weights_label\n",
    "        return image,mask_label\n",
    "\n",
    "\n",
    "def get_binary_mask(mask,threshold):\n",
    "    return np.where(mask > threshold, 1, 0)\n",
    "    \n",
    "class InhouseGetData_v2(InhouseGetData):\n",
    "    def __init__(self,image_filepaths,oracle_results_thresholds,image_transform,data_aug=True,has_weights=True):\n",
    "        InhouseGetData.__init__(self,image_filepaths,image_transform,data_aug,has_weights)\n",
    "        self.oracle_results_thresholds = oracle_results_thresholds\n",
    "    def __getitem__(self,idx):\n",
    "        filepath = self.image_filepaths[idx]\n",
    "        #print(\"Filepath: \" + filepath)\n",
    "        arr_and_mask = np.load(filepath)\n",
    "        copy_arr_mask = arr_and_mask.copy()\n",
    "        if self.data_aug:\n",
    "            copy_arr_mask = random_flip(copy_arr_mask, 0, True)\n",
    "            copy_arr_mask = random_flip(copy_arr_mask, 1, True)\n",
    "            copy_arr_mask = random_rotate_90(copy_arr_mask, True)\n",
    "            copy_arr_mask = random_rotate_90(copy_arr_mask, True)\n",
    "            copy_arr_mask = random_rotate_90(copy_arr_mask, True)\n",
    "        arr = copy_arr_mask[0,:,:].copy()\n",
    "        mask = copy_arr_mask[1,:,:].copy()\n",
    "        #apply threshold to mask\n",
    "        threshold = self.oracle_results_thresholds[(\"/\".join(filepath.split(\"/\")[-2:]))[:-4]]\n",
    "        mask = get_binary_mask(mask,threshold)\n",
    "        if self.has_weights:\n",
    "            weights = copy_arr_mask[2,:,:].copy()\n",
    "        arr = exposure.equalize_hist(arr) #histogram equalization, remove if u want\n",
    "        \n",
    "        image = self.image_transform(arr)\n",
    "        mask_label = self.image_transform(mask)\n",
    "        if self.has_weights:\n",
    "            weights_label = self.image_transform(weights)\n",
    "        if self.has_weights:\n",
    "            return image,mask_label,weights_label\n",
    "        return image,mask_label\n",
    "        \n",
    "    \n",
    "def unet_dataloader_v2(saved_oracle_filepaths,oracle_results_thresholds,batch_size=8,num_workers=2):\n",
    "    transforms_arr = [transforms.ToTensor(),transforms.Resize((256,256))]\n",
    "    transform = transforms.Compose(transforms_arr)\n",
    "    trainset = InhouseGetData_v2(saved_oracle_filepaths,oracle_results_thresholds,transform,data_aug=True,has_weights=True)\n",
    "    trainloader = DataLoader(trainset,batch_size=batch_size,num_workers=num_workers)\n",
    "    return trainloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a7e9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model_path = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_03_17_a/Iter1/unetmodel.pth\"\n",
    "custom_load = True\n",
    "if custom_load:\n",
    "    model_path = custom_model_path\n",
    "    unet_model = torch.load(model_path)\n",
    "    \n",
    "else:\n",
    "    #Load in/Initialize a UNet model for retraining\n",
    "\n",
    "    from_scratch = False\n",
    "    #Define the initial UNet model (iter0)\n",
    "    if iter_num==1:\n",
    "        model_path = \"/usr/xtmp/vs196/mammoproj/SavedModels/HyperparameterUNet_nobuffer/unet_5_0.5/Model/unetmodel_FINAL.pth\"\n",
    "    else:\n",
    "        model_path = model_save_path\n",
    "\n",
    "    if not from_scratch:\n",
    "        unet_model = torch.load(model_path)\n",
    "    else:\n",
    "        unet_model = getattr(ternausnet.models, \"UNet16\")(num_classes=2,pretrained=True).cuda()\n",
    "\n",
    "total_loss_tracker = []\n",
    "total_metric_tracker = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f70c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handles three channels: [data,floodfilled binary mask,border]\n",
    "def redirect_saved_oracle_filepaths_to_thresheld_directory(saved_oracle_filepaths,im_dir):\n",
    "    new_filepaths = [(im_dir + \"/\".join(filepath.split(\"/\")[-2:])) for filepath in saved_oracle_filepaths]\n",
    "    return new_filepaths\n",
    "\n",
    "new_saved_oracle_filepaths = redirect_saved_oracle_filepaths_to_thresheld_directory(saved_oracle_filepaths, im_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcf181ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_03_10_a/Iter1/OracleThresholdedImages_ff/\n",
      "03_10_a\n"
     ]
    }
   ],
   "source": [
    "# print(saved_oracle_filepaths[0])\n",
    "# print(new_saved_oracle_filepaths[0])\n",
    "print(im_dir)\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55fa21ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [02:01<00:00,  1.27it/s]\n",
      "100%|██████████| 155/155 [02:01<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "#Retrain UNet with training data taken from OracleImages/Iter\n",
    "#new_unet_dataloader = unet_dataloader(saved_oracle_filepaths,8,2)\n",
    "new_unet_dataloader = unet_dataloader(new_saved_oracle_filepaths,8,2)\n",
    "cbis_trainloader,_ = CBIS_DDSM_get_DataLoader(8,2)\n",
    "\n",
    "#unet_model,loss_tracker,metric_tracker = unet_update_model(unet_model,cbis_trainloader,num_epochs=15)\n",
    "unet_model,loss_tracker,metric_tracker = unet_update_model_multi_dataloader(unet_model,new_unet_dataloader,cbis_trainloader,num_epochs=2)\n",
    "total_loss_tracker.extend(loss_tracker)\n",
    "total_metric_tracker.extend(metric_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d75650c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x154c20188550>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(total_loss_tracker)\n",
    "plt.plot(total_metric_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "506f155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/824 [00:00<00:38, 21.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging. Data shape is: (256, 256), and unbinarized shape is: (256, 256), and unbin output shape is torch.Size([1, 256, 256])\n",
      "Debugging. Data shape is: (256, 256), and unbinarized shape is: (256, 256), and unbin output shape is torch.Size([1, 256, 256])\n",
      "Debugging. Data shape is: (256, 256), and unbinarized shape is: (256, 256), and unbin output shape is torch.Size([1, 256, 256])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c3bd7fae30ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Saves all segmentations (preserved for images labelled correctly by the oracle, new for all others) to save_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Have different subfolders for different iterations of unet update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mevaluate_model_on_new_segmentations_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegmentation_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msaved_oracle_filepaths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorrect_save_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#save_dir is what we convert to floodfill two cells down.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/project/xtmp/vs196/mammoproj/Code/ActiveLearning/unet.py\u001b[0m in \u001b[0;36mevaluate_model_on_new_segmentations_and_save\u001b[0;34m(model, segmentation_folder, saved_oracle_filepaths, correct_save_dir, save_dir, iter_num)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Debugging. Data shape is: {detach_image.shape}, and unbinarized shape is: {unbinarized_unet_seg.shape}, and unbin output shape is {unbin_output.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatient_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdetach_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munbin_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_oracle_save_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdetach_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munbinarized_unet_seg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m#if normal, save it to save_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/project/xtmp/vs196/mammoproj/Env/trainenv/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "#evaluate new segmentations for ALL images \n",
    "#Save SAVED_ORACLE_FILEPATH NEW segmentations to SEPARATE FOLDER for VIEWING ONLY\n",
    "#Save this iteration saved_oracle_filepaths along with new segmentations for all other patients\n",
    "segmentation_folder = unbin_im_dir\n",
    "correct_save_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/UNetSegmentations_C/\"\n",
    "save_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/UNetSegmentations/\"\n",
    "#Method creates new segmentations using updated model. \n",
    "#Saves new segmentations for images labelled correctly by the oracle to correct_save_dir\n",
    "#Saves all segmentations (preserved for images labelled correctly by the oracle, new for all others) to save_dir\n",
    "#Have different subfolders for different iterations of unet update\n",
    "evaluate_model_on_new_segmentations_and_save(unet_model,segmentation_folder,saved_oracle_filepaths,correct_save_dir,save_dir,iter_num)\n",
    "\n",
    "#save_dir is what we convert to floodfill two cells down.\n",
    "#TODO: ADD TQDM TO EVALUATE_MODEL...\n",
    "#TODO: Add flexibility for alina saving (add alina paths)\n",
    "#TODO: Update evaluate model to output two channels where the first is the same as now and the second is a continuous version of the mask.\n",
    "#Look at bad oracles and see if those got better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b06bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model_save_path = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/unetmodel.pth\"\n",
    "torch.save(unet_model,model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb739f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_02_28_a/Iter1/UNetSegmentations\"\n",
    "iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39ca29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [05:29<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#Convert initial UNet Segmentations to flood-filled and resave\n",
    "#raw_segmentation_folder = \"/usr/xtmp/mammo/image_datasets/data_split_july2021/square_ROI_by_shape_segmentations/train/\"\n",
    "raw_segmentation_folder = save_dir #remove for rerun\n",
    "im_dir = convert_directory_to_floodfill(raw_segmentation_folder,iter0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd7cf395",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_11_29_a/Iter1/UNetSegmentations_ff/\"\n",
    "im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_02_28_a/Iter1/UNetSegmentations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading iter 1 purposes\n",
    "iter_num = 2\n",
    "im_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_11_23_a/Iter1/UNetSegmentations_ff/\"\n",
    "dataloader = get_DataLoader(im_dir,32,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e566dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(iter_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f94a622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_03_10_a/Iter1/UNetSegmentations_ff/\n"
     ]
    }
   ],
   "source": [
    "print(im_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4ce2a",
   "metadata": {},
   "source": [
    "# Saving Images for Fides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41d54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_annotation(filepath,annotations):\n",
    "    ncols, nrows = 3, len(filepath)\n",
    "    fig = plt.figure(constrained_layout=False)\n",
    "    fig.set_size_inches(9, 3*len(filepath)+1)\n",
    "    fig.tight_layout()\n",
    "    spec = gridspec.GridSpec(ncols=ncols, nrows=nrows, figure=fig,hspace=0,wspace=0)\n",
    "    \n",
    "    anno_opts = dict(xy=(0.05, 0.05), xycoords='axes fraction', va='bottom', ha='left',color='cyan',fontweight='extra bold',fontsize='8')\n",
    "\n",
    "    f_axes = []\n",
    "    for row in range(nrows):\n",
    "        f_axes.append([])\n",
    "        for col in range(ncols):\n",
    "            f_axes[-1].append(fig.add_subplot(spec[row, col]))\n",
    "\n",
    "    for ax_num, ax in enumerate(f_axes[0]):\n",
    "            if ax_num == 0:\n",
    "                ax.set_title(\"Image\", fontdict=None, loc='left', color = \"k\")\n",
    "            elif ax_num == 1:\n",
    "                ax.set_title(\"Segmentation\", fontdict=None, loc='left', color = \"k\")\n",
    "            elif ax_num == 2:\n",
    "                ax.set_title(\"Overlay\", fontdict=None, loc='left', color = \"k\")\n",
    "\n",
    "    for row in range(nrows):\n",
    "        image_and_mask = np.load(filepath[row])\n",
    "        f_axes[row][0].imshow(image_and_mask[0],cmap='gray')\n",
    "        f_axes[row][0].set_axis_off()\n",
    "        \n",
    "        f_axes[row][1].imshow(image_and_mask[1],cmap='gray')\n",
    "        f_axes[row][1].set_axis_off()\n",
    "\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255*(1-image_and_mask[1])), cv2.COLORMAP_AUTUMN)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        heatmap = heatmap[...,::-1]\n",
    "\n",
    "        img = 0.6 * np.stack([image_and_mask[0],image_and_mask[0],image_and_mask[0]],axis=-1) + 0.3*heatmap\n",
    "        f_axes[row][2].imshow(img)\n",
    "        f_axes[row][2].set_axis_off()\n",
    "        \n",
    "        f_axes[row][0].annotate(annotations[row],**anno_opts)\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobechecked_save_dir = \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/ToBeChecked_neworder/\"\n",
    "\n",
    "exclude_data_from_dirs = [\"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_11_30_a/Iter1/CorrectSegmentations/\",\n",
    "                          \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_12_19_b/Iter1/CorrectSegmentations/\",\n",
    "                          \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_12_21_b/Iter1/CorrectSegmentations/\",\n",
    "                          \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_12_21_b/Iter2/CorrectSegmentations/\",\n",
    "                          \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_01_01_a/Iter1/CorrectSegmentations/\"\n",
    "                         ]\n",
    "\n",
    "exclude_data_from_dirs = [\"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_01_01_b/Iter1/CorrectSegmentations/\",\n",
    "                          \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_12_21_b/Iter1/CorrectSegmentations/\"\n",
    "                         ]\n",
    "\n",
    "exclude_patientIDs = set()\n",
    "for exclude_data_from_dir in exclude_data_from_dirs:\n",
    "    already_sent_oracle_filepaths = pickle.load(open(exclude_data_from_dir + \"saved_data_struct/\" + \"Oracle_Filepaths.pickle\",'rb'))\n",
    "    print(len(already_sent_oracle_filepaths))\n",
    "    for already_sent_oracle_filepath in already_sent_oracle_filepaths:\n",
    "        pat_ID = already_sent_oracle_filepath.split('/')[-1][:-4]\n",
    "        exclude_patientIDs.add(pat_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(exclude_patientIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6f4d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collision_cnt = 0\n",
    "for save_filepath in saved_oracle_filepaths:\n",
    "    #plot the 3 part image as in oracle query\n",
    "    annotation = '/'.join((save_filepath.split('/'))[-2:])\n",
    "    pat_ID = save_filepath.split('/')[-1][:-4]\n",
    "    if pat_ID in exclude_patientIDs:\n",
    "        print(f'collision for {pat_ID}')\n",
    "        collision_cnt += 1\n",
    "    else:\n",
    "        display_image_annotation([save_filepath],[annotation])\n",
    "\n",
    "        #put an annotation on that image with the patient ID and the class\n",
    "        #save the image in tobechecked_save_dir\n",
    "        tobechecked_save_subfolder = os.path.join(tobechecked_save_dir ,save_filepath.split('/')[-2]) + \"/\"\n",
    "        if not os.path.exists(tobechecked_save_subfolder):\n",
    "            os.makedirs(tobechecked_save_subfolder)\n",
    "        save_path = os.path.join(tobechecked_save_dir, annotation)[:-4]+\".png\"\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "print(f\"{collision_cnt} collisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tobechecked_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(image1,image2):\n",
    "    display_image_annotation([image1,image2],[image1,image2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c330590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6b04f9d",
   "metadata": {},
   "source": [
    "# Vivek GAN stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717513e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vivek GAN Playground\n",
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "#from models import G, D, weights_init (Already imported)\n",
    "#from data import get_training_set, get_test_set\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "#Retrain UNet with training data taken from OracleImages/Iter\n",
    "new_unet_dataloader = unet_dataloader(saved_oracle_filepaths,8,2)\n",
    "#cbis_trainloader,_ = CBIS_DDSM_get_DataLoader(8,2)\n",
    "\n",
    "#Setup code\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    return loss.mean()\n",
    "\n",
    "cudnn.benchmark = True\n",
    "training_data_loader = new_unet_dataloader #replace with data loader from above (in UNET)\n",
    "\n",
    "#DEFINE BATCH_SIZE\n",
    "batch_size = 8\n",
    "\n",
    "#Initialize model and initialization values\n",
    "input_nc = 1\n",
    "output_nc = 1\n",
    "ngf = ndf = 32\n",
    "netG = G(input_nc, output_nc, ngf)\n",
    "netG.apply(weights_init)\n",
    "netD = D(input_nc, output_nc, ndf)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion_l1 = nn.L1Loss()\n",
    "criterion_mse = nn.MSELoss()\n",
    "\n",
    "real_A = torch.FloatTensor(batch_size, input_nc, 256, 256)\n",
    "real_B = torch.FloatTensor(batch_size, output_nc, 256, 256)\n",
    "label = torch.FloatTensor(batch_size)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "#Push everything onto CUDA\n",
    "netD = netD.cuda()\n",
    "netG = netG.cuda()\n",
    "criterion = criterion.cuda()\n",
    "criterion_l1 = criterion_l1.cuda()\n",
    "criterion_mse = criterion_mse.cuda()\n",
    "real_A = real_A.cuda()\n",
    "real_B = real_B.cuda()\n",
    "label = label.cuda()\n",
    "\n",
    "real_A = Variable(real_A)\n",
    "real_B = Variable(real_B)\n",
    "label = Variable(label)\n",
    "\n",
    "#Setup ADAM optimizer - REPLACE\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "lamb = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "#Training Code\n",
    "for iteration, batch in enumerate(tqdm(training_data_loader), 1):\n",
    "    ############################\n",
    "    # (1) Update D network: maximize log(D(x,y)) + log(1 - D(x,G(x)))\n",
    "    ###########################\n",
    "    # train with real\n",
    "    netD.volatile = True\n",
    "    netD.zero_grad()\n",
    "    print(batch[0].shape)\n",
    "    with torch.no_grad():\n",
    "        real_a_cpu, real_b_cpu = batch[0], batch[1]\n",
    "        real_A.resize_(real_a_cpu.size()).copy_(real_a_cpu)\n",
    "        real_B.resize_(real_b_cpu.size()).copy_(real_b_cpu)\n",
    "\n",
    "    output = netD(torch.cat((real_A, real_B), 1))\n",
    "    with torch.no_grad():\n",
    "        label.resize_(output.size()).fill_(real_label)\n",
    "    err_d_real = criterion(output, label)\n",
    "    # print (err_d_real)\n",
    "    err_d_real.backward()\n",
    "    d_x_y = output.data.mean()\n",
    "\n",
    "    # train with fake\n",
    "    fake_b = netG(real_A)\n",
    "    output = netD(torch.cat((real_A, fake_b.detach()), 1))\n",
    "    with torch.no_grad():\n",
    "        label.resize_(output.size()).fill_(fake_label)\n",
    "    err_d_fake = criterion(output, label)\n",
    "    # print (err_d_fake)\n",
    "    err_d_fake.backward()\n",
    "    d_x_gx = output.data.mean()\n",
    "\n",
    "    err_d = (err_d_real + err_d_fake) / 2.0\n",
    "    optimizerD.step()\n",
    "\n",
    "    ############################\n",
    "    # (2) Update G network: maximize log(D(x,G(x))) + L1(y,G(x))\n",
    "    ###########################\n",
    "    netG.zero_grad()\n",
    "    netD.volatile = True\n",
    "    output = netD(torch.cat((real_A, fake_b), 1))\n",
    "    label.data.resize_(output.size()).fill_(real_label)\n",
    "    err_g = criterion(output, label) + lamb * dice_loss(fake_b, real_B)\n",
    "    err_g.backward()\n",
    "    d_x_gx_2 = output.data.mean()\n",
    "    optimizerG.step()\n",
    "    \n",
    "    #Print epoch info\n",
    "    print(\"===> Epoch[{}]({}/{}): Loss_D: {:.4f} Loss_G: {:.4f} D(x): {:.4f} D(G(z)): {:.4f}/{:.4f}\".format(\n",
    "            epoch, iteration, len(training_data_loader), err_d.data[0], err_g.data[0], d_x_y, d_x_gx, d_x_gx_2))\n",
    "\n",
    "\n",
    "#Eval Code\n",
    "\n",
    "#Run Code:\n",
    "#modelD, modelG, criterionD, criterionG, metric_trackerGD = vivek.model_update(modelD, modelG, criterionD, criterionG, dataloader,num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to save segmentations at correct thresholds for use in UNEt training on the same stage\n",
    "\n",
    "#Takes in correct oracle results and thresholds and save path\n",
    "#First threshold it to a binary mask\n",
    "\n",
    "def threshold_and_save_images(saved_oracle_filepaths, oracle_results_thresholds, save_dir):\n",
    "    for filepath in tqdm(saved_oracle_filepaths):\n",
    "        threshold = oracle_results_thresholds[(\"/\".join(filepath.split(\"/\")[-2:]))[:-4]]\n",
    "        arr_and_mask = np.load(filepath)\n",
    "        copy_arr_mask = arr_and_mask.copy()\n",
    "\n",
    "        arr = copy_arr_mask[0,:,:].copy()\n",
    "        mask = copy_arr_mask[1,:,:].copy()\n",
    "        #apply threshold to mask\n",
    "        mask = get_binary_mask(mask, threshold)\n",
    "        to_save = np.stack([arr, mask])\n",
    "        \n",
    "        save_save_dir = save_dir + \"/\".join(filepath.split(\"/\")[-2:])\n",
    "        if not os.path.exists(save_dir + filepath.split(\"/\")[-2]):\n",
    "            os.makedirs(save_dir + filepath.split(\"/\")[-2])\n",
    "        np.save(save_save_dir, to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb06a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save thresholded\n",
    "if True:\n",
    "    if users_name == 'alina':\n",
    "        save_dir = \"/usr/xtmp/mammo/alina_code/shapesAL/data/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/OracleThresholdedImages/\"\n",
    "    elif users_name == 'vaibhav':\n",
    "        save_dir = \"/usr/xtmp/vs196/mammoproj/Code/ActiveLearning/AllOracleRuns/Run_\" + run_id + \"/Iter\" + str(iter_num) + \"/OracleThresholdedImages/\"\n",
    "    else:\n",
    "        print(\"wrong username\")\n",
    "        \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    else:\n",
    "        user_input = input(\"Do you want to overwrite this directory? Type y or yes to continue\")\n",
    "        if not (user_input==\"y\" or user_input==\"yes\"):\n",
    "            assert(False)\n",
    "            \n",
    "    threshold_and_save_images(saved_oracle_filepaths, oracle_results_thresholds, save_dir)\n",
    "    save_dir = convert_directory_to_floodfill(save_dir,iter0=False)\n",
    "    print(f\"Saved in {save_dir}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f74ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
